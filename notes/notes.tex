\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, dsfont, physics, hyperref, cleveref}
\usepackage[margin=1.5cm]{geometry}
\usepackage{biblatex}

\title{Notes}
\author{SCK team}
\date{}

\newtheorem{proposition}{Proposition}

\addbibresource{library.bib}

\begin{document}
    \maketitle

\section{Notation}
\begin{itemize}
    \item Average emission per source: $x_1,\dots,x_m$ where $m = 200$.
    \item Measured concentrations $y_1,\dots,y_n$ where $n = 4636$. These are consecutive measurements for several measurement stations. 
    \item $M_{i,(j,\Delta)}$ is a block in the sensitivity matrix $M$ that represents the contribution of $x_j$ from $\Delta - 1$ days ago to the observed concentration $y_i$.
    \item Scaling factors $s_1,\dots,s_m$ (in \texttt{scalings.csv}). These are put in a diagonal matrix $S$.
    \item $\vb{1}_q = [1\,\, \dots\,\, 1]^T \in \mathbb{R}^q$ is the constant vector consisting of only ones.
    \item $\odot$ is the Hadamard (i.e.\, elementwise) product, given by $(AB)_{ij} = A_{ij} B_{ij}$.
    \item $\mathrm{diag}$ is the canonical identification of $p$-vectors and $p \times p$ diagonal matrices, i.e.,
    $$
    \mathrm{diag}\left(
    \begin{bmatrix}
        a_1 \\ 
        \vdots \\
        a_p
    \end{bmatrix}
    \right)
    =
    \begin{bmatrix}
        a_1 & 0 & \dots & 0 \\
        0 & a_2 & \dots & 0 \\
        \vdots & \vdots & \ddots & 0 \\
        0 & 0 & \cdots & a_p
    \end{bmatrix}
    .$$
    \item $\otimes$ is the Kronecker product. In particular,
        $$
        \underbrace{
        \begin{bmatrix}
            a_1 \\
            \vdots \\
            a_p
        \end{bmatrix}
        }_{\in \mathbb{R}^{p}}
        \otimes 
        \underbrace{
        \begin{bmatrix}
            1 \\
            \vdots \\
            1
        \end{bmatrix}
        }_{\in \mathbb{R}^{q}}
        =
        \underbrace{
        \begin{bmatrix}
            a_1 \\
            \vdots \\
            a_1 \\
            a_2 \\
            \vdots \\
            a_2 \\
            \vdots \\
            a_p
        \end{bmatrix}
        }_{\in \mathbb{R}^{pq}}
        .$$
        A standard rule of thumb in numerical linear algebra is that the Kronecker product serves a mostly theoretical purpose and is rarely calculated explicitly. See, e.g. \cite[Section 12.3]{Golub2013}.
\end{itemize}

\section{Model}
\subsection{Initial model}
With $n=1,\dots,4636$ and $m = 1,\dots,200$, the initial model is
$$
y_i \approx
\hat{y}_i = \sum_{j=1}^m \sum_{\Delta = 1}^{15}  M_{i,(j,\Delta)} \frac{x_{j}}{s_j}
.$$
That is, for each emission source $j$, we simulate the contribution of 15 days, and add this together for all $j$. A matrix formulation of this reads as follows:
\begin{align}
    \label{eq: initial model concise} 
\begin{bmatrix}
    \hat{y}_1 \\ 
    \vdots \\
    \hat{y}_{n}
\end{bmatrix}
&=
\begin{bmatrix}
    M_{1,(1,1)} & M_{1,(1,2)} & \cdots & M_{1,(m,15)} \\
    \vdots & \vdots & & \vdots \\
    M_{n,(1,1)} & M_{n,(1,2)} & \cdots & M_{n,(m,15)} \\
\end{bmatrix}
\begin{bmatrix}
    s_1^{-1} x_1\\
    \vdots \\
    s_1^{-1} x_1 \\
    s_2^{-1} x_2 \\
    \vdots \\
    s_n^{-1} x_m
\end{bmatrix} 
\\
&=
\begin{bmatrix}
    M_{1,(1,1)} & M_{1,(1,2)} & \cdots & M_{1,(n,15)} \\
    \vdots & \vdots & & \vdots \\
    M_{m,(1,1)} & M_{m,(1,2)} & \cdots & M_{m,(n,15)} \\
\end{bmatrix}
\left( 
     \begin{bmatrix}
        s_1 & & \\
         & \ddots & \\
        & & s_n
     \end{bmatrix}^{-1}
     \begin{bmatrix}
        x_1 \\
        \vdots \\
        x_n
     \end{bmatrix}
     \otimes \vb{1}_{15}
\right)
\nonumber
.\end{align}

Now we present a different interpretation. Instead of $S^{-1} x \otimes \vb{1}_{15}$, we make a vector $\tilde{x} := S^{-1} x \otimes \vb{1}_{365}$. This is a vector of length $365 \times 200$ that gives the emission from day 1 to day 365 for each source. This is assumed to be constant for all days. Therefore, $\tilde{x}$ consists of $m = 200$ blocks, each of which is a constant vector of length $365$.

We can write a system of equations that is logically the same as \cref{eq: initial model concise} but uses $\tilde{x}$. This gives a larger linear system. Because the emission from most dates has a negligible contribution to the observed concentrations, many coefficients in the larger matrix will be zero. For every measurement $i$, we have a linear relationship that looks something like this:
$$
        \hat{y}_i
    =
    \begin{bmatrix}
        \cdots & M_{i,(1,1)} & \dots & M_{i,(1,15)} & 0 & \dots & M_{i,(2,1)} & \dots & M_{i,(2,15)} & \dots
    \end{bmatrix}
    (S^{-1} x \otimes \vb{1}_{365})
.
$$
The presence of zeros in the matrix indicates that emissions from the distant past do not contribute to the observed concentrations.

Now assume that in each station, there is one observation per day, named $y_{kt}$, where $k = 1,\dots,K$ where $K$ is the number of stations and $t = 1,\dots,365$. The index $t=1$ corresponds to the most recent measurement. Then the full system reads as follows:
\begin{equation}
    \label{eq: initial model 365 days}
\begin{bmatrix}
    \hat{y}_{1,1} \\
    \hat{y}_{1,2} \\
    \vdots \\
    \hat{y}_{2,1} \\ 
    \vdots \\
    \hat{y}_{K,365}
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
    M_{(1,1), (1,1)} & M_{(1,1), (1,2)} & \cdots & M_{(1,1),(1,15)} & 0 & \cdots & \cdots & M_{(1,1), (2,1)} & \cdots & 0 \\
    0 & M_{(1,2), (1,1)} & \cdots & M_{(1,2), (1,14)} & M_{(1,2), (1,15)} & 0 & \cdots & 0 & \cdots & 0 \\
    \vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots & & \vdots \\
    M_{(2,1), (1,1)} & M_{(2,1), (1,2)} & \cdots & M_{(2,1),(1,15)} & 0 & \cdots & \cdots & M_{(2,1), (2,1)} & \cdots & 0 \\
    \vdots \\
    \vdots
\end{bmatrix}
}_{ := M_{shift}}
\tilde{x}
\end{equation}
where 
$$
\tilde{x} := S^{-1} x \otimes \vb{1}_{365}
=
\begin{bmatrix}
    s_1^{-1} x_1 \\
    \vdots \\
    s_1^{-1} x_1 \\ 
    s_2^{-1} x_2 \\
    \vdots \\ 
    s_m^{-1} x_m
\end{bmatrix}
.$$

We could reorganise the vector $\tilde{x}$ to the transposed expression $\vb{1}_{365} \otimes (S^{-1} x)$, which is a vector containing all emissions on day 1, followed by all emissions on day 2, etc. A similar permutation of $y$ could be chosen. This has the effect of permuting the rows and columns of $M_{shift}$. A permutation of the variables gives a model that is essentially the same as \cref{eq: initial model 365 days} but may have advantages when it comes to implementation or computational efficiency. We will not consider this.

\subsection{Model with time-dependent correction to the emissions}
If we weigh each day and each emission source by some factor, we are applying a transformation
$$
\begin{bmatrix}
    s_1^{-1} x_1 \\
    \vdots \\
    s_1^{-1} x_1 \\ 
    s_2^{-1} x_2 \\
    \vdots \\ 
    s_m^{-1} x_m
\end{bmatrix}
\mapsto
\begin{bmatrix}
    w_{1,1} s_1^{-1} x_1 \\
    \vdots \\
    w_{1,365} s_1^{-1} x_1 \\ 
    w_{2,1} s_2^{-1} x_2 \\
    \vdots \\ 
    w_{m, 365} s_m^{-1} x_m
\end{bmatrix}
$$
which can be written concisely as 
$$
W (S^{-1} x \otimes \vb{1}_{365})
\quad\text{where}\quad 
W = \begin{bmatrix}
    w_{1,1} & 0 & \cdots & 0 \\
    0 & w_{1,2} & \cdots & 0 \\
    \vdots & \vdots & \ddots & 0 \\
    0 & 0 & \cdots & w_{m, 365}
\end{bmatrix}
.$$
Using this weighted vector of emissions instead of $\tilde{x}$ in \cref{eq: initial model 365 days} gives
\begin{equation}
    \label{eq: time-evolving model concise}
\hat{y} = M_{shift} W (S^{-1} x \otimes \vb{1}_{365})
.\end{equation}
In this expression, the unknown is $W$ and should be fit to the data. 
% There are a plethora of numerical techniques for solving linear equations where the unknown is a matrix.
% In the following, we provide a theoretical description of a system that can be solved for $W$.

% We can put constraints on $W$ of the form $W \in \mathcal{W}$, where $\mathcal{W}$ is some set of constrained diagonal matrices. Suppose we have a parametrisation $\Gamma: \mathbb{R}^{k} \rightarrow \mathcal{W}, u \mapsto \Gamma(u)$. 
% In this case, \cref{eq: time-evolving model concise} can be reformulated as a linear operation applied to $\Gamma(u)$ as follows:
% $$
% \hat{y} = ((S^{-1} x \otimes \vb{1}_{365})^T \otimes M_{shift}) \mathrm{vec}\, \Gamma(u)
% .$$
% If $\Gamma$ is a linear map, the above is a linear expression in $u$. Then, the minimisation of $\norm{y - \hat{y}}^2$ can be solved using numerical linear algebra.

\subsection{Fitting the model}
The indices $i = 1,\dots,n$ are partitioned as a disjoint union $\mathcal{I}_{det} \cup \mathcal{I}_{0}$ where $\mathcal{I}_0$ contains the indices of non-detection events. Let $P_{det}$ be the linear map that removes the rows of a matrix that correspond to $\mathcal{I}_0$. We can measure the accuracy as $\norm{\log.(P_{det} \hat{y})}^2$ where $\log.(\cdot)$ is the elementwise \emph{natural} logarithm. The simplicity of $W$ can be measured as $\norm{\log W}^2$. If we forget about non-detections, we have the cost function 
$$
\min_{W \text{ diagonal}} f(W) =
\underbrace{\frac{1}{2}\norm{\log.(P_{det} M_{shift} W \tilde{x})}^2}_{\text{prediction error}} + \underbrace{\frac{1}{2}\lambda \norm{\log W}^2}_{\text{deviation from average emission}}
.$$
\begin{proposition}
    Let $\hat{z}$ be the vector $[\hat{z}_1\,\,\dots\,\,\hat{z}_n]^T$ where 
    $$
    \hat{z}_i = \begin{cases}
        \frac{\log \hat{y}_i}{\hat{y}_i} & \text{if observation $i$ is a detection} \\
        0 & \text{if observation $i$ is a non-detection} 
    \end{cases}
    .$$
    Then the gradient of $f(W)$ is the diagonal matrix
    $$
    \mathrm{grad}_{W} f(W) = \mathrm{diag}\left(\left(M_{shift}^T \hat{z}\right) \odot \tilde{x}\right) + \lambda W^{-1} \log W
    .$$
\end{proposition}
\begin{proof}

Parameterise $W$ linearly as $\mathrm{vec}\, W(t) = U t$ for some unitary $U$.
Define $$
G(t) :=
P_{det} \hat{y}_i
=
P_{det} M_{shift} W(t) \tilde{x}
\quad\text{and}\quad
F(t) := \log.(G(t))
.$$
Then 
$$
G(t) = P_{det} M_{shift} W(t) \tilde{x} = \underbrace{(\tilde{x}^T \otimes P_{det}M_{shift}) U}_A t
.$$
Since $G(t)$ is linear, $F(t) = \log.(G(t))$ is easy to differentiate:
$$
\frac{\partial}{\partial t} F(t) =
\frac{\partial}{\partial t} \log.(G(t)) = \mathrm{diag}\,(G(t))^{-1} A
.$$
Write $z := P_{det} \hat{z}$.
We can interpret $z$ as the vector obtained by applying the map $x \mapsto (\log x) / x$ elementwise to $G(t)$.
Using standard differentiation rules for the squared norm of a vector-valued function, we obtain
\begin{align*}
\mathrm{grad}_t\, \left(\frac{1}{2} \norm{F(t)}^2 \right) = \left(\frac{\partial F}{\partial t}\right)^T F(t)
&= A^T \mathrm{diag}(G(t))^{-1} \log.(G(t)) \\
&= A^T z \\
&= U^T (\tilde{x} \otimes (P_{det} M_{shift})^T) z \\
&= U^T (P_{det} M_{shift})^T z \tilde{x}^T \\
&= U^T M_{shift}^T P_{det}^T P_{det} \hat{z} \tilde{x}^T \\
&= U^T M_{shift}^T \hat{z} \tilde{x}^T
\end{align*}
where the last equality follows from the fact that $P_{det}^T P_{det}$ is the map that sets the rows corresponding to the non-detection events to zero. 
Since $U$ is unitary, we have $$
\mathrm{grad}_W \left(\frac{1}{2} \norm{F(t)}^2 \right)
=
U \mathrm{grad}_t\, \left(\frac{1}{2} \norm{F(t)}^2 \right)
=
UU^T M_{shift}^T \hat{z} \tilde{x}^T
$$
The linear map $UU^T$ is simply the orthogonal projector onto the diagonal matrices. This orthogonal projection simply sets the off-diagonal elements of its argument to zero. Since the diagonal of $M_{shift}^T \hat{z} \tilde{x}^T$ is $(M_{shift}^T \hat{z}) \odot \tilde{x}$, we have 
$$
\mathrm{grad}_W \left(\frac{1}{2} \norm{F(t)}^2 \right)
=
\mathrm{diag}((M_{shift}^T \hat{z}) \odot \tilde{x})
.$$

The foregoing gives the derivative of the first term in $f(W)$. The gradient of the second term can be calculated in exactly the same manner: 
$$
\mathrm{grad}_W\, \left(\frac{1}{2}\norm{\log W}^2\right) = W^{-1} \log W
.$$
\end{proof}
For even more evidence, there is a Julia script in the repository that checks this formula against the result obtained from automatic differentiation (using simplified mock data). The result is correct up to machine precision.


\printbibliography

\end{document}

