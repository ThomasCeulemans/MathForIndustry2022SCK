\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, dsfont, physics, hyperref, cleveref}
\usepackage[margin=1.5cm]{geometry}
\usepackage{biblatex}

\title{Notes}
\author{SCK team}
\date{}

\newtheorem{proposition}{Proposition}

\addbibresource{library.bib}

\begin{document}
    \maketitle

\section{Notation}
\begin{itemize}
    \item Average emission per source: $x_1,\dots,x_m$ where $m = 200$.
    \item Measured concentrations $y_1,\dots,y_n$ where $n = 4636$. These are consecutive measurements for several measurement stations. 
    \item $M_{i,(j,\Delta)}$ is a block in the sensitivity matrix $M$ that represents the contribution of $x_j$ from $\Delta - 1$ days ago to the observed concentration $y_i$.
    \item Scaling factors $s_1,\dots,s_m$ (in \texttt{scalings.csv}). These are put in a diagonal matrix $S$.
    \item $\vb{1}_q = [1\,\, \dots\,\, 1]^T \in \mathbb{R}^q$ is the constant vector consisting of only ones.
    \item $\odot$ is the Hadamard (i.e.\, elementwise) product, given by $(AB)_{ij} = A_{ij} B_{ij}$.
    \item $\mathrm{vec}: \mathbb{R}^{p \times q} \rightarrow \mathbb{R}^{pq}$ is the vectorisation operator. It can be understood as stacking the columns of a matrix in one long vector. 
    \item $\mathrm{diag}$ is the canonical identification of $p$-vectors and $p \times p$ diagonal matrices, i.e.,
    $$
    \mathrm{diag}\left(
    \begin{bmatrix}
        a_1 \\ 
        \vdots \\
        a_p
    \end{bmatrix}
    \right)
    =
    \begin{bmatrix}
        a_1 & 0 & \dots & 0 \\
        0 & a_2 & \dots & 0 \\
        \vdots & \vdots & \ddots & 0 \\
        0 & 0 & \cdots & a_p
    \end{bmatrix}
    .$$
    \item $\otimes$ is the Kronecker product. In particular,
        $$
        \underbrace{
        \begin{bmatrix}
            a_1 \\
            \vdots \\
            a_p
        \end{bmatrix}
        }_{\in \mathbb{R}^{p}}
        \otimes 
        \underbrace{
        \begin{bmatrix}
            1 \\
            \vdots \\
            1
        \end{bmatrix}
        }_{\in \mathbb{R}^{q}}
        =
        \underbrace{
        \begin{bmatrix}
            a_1 \\
            \vdots \\
            a_1 \\
            a_2 \\
            \vdots \\
            a_2 \\
            \vdots \\
            a_p
        \end{bmatrix}
        }_{\in \mathbb{R}^{pq}}
        .$$
        A standard rule of thumb in numerical linear algebra is that the Kronecker product serves a mostly theoretical purpose and is rarely calculated explicitly.
        This is because the Kronecker product has the following convenient property \cite[Section 12.3]{Golub2013}: $\mathrm{vec} (AXB) = (B^T \otimes A) \mathrm{vec} X$ for all matrices $A,X,B$.
    \item For a vector-valued function $F(x)$, the Jacobian matrix is denoted as $\frac{\partial F}{\partial x}$.
\end{itemize}

\section{Model}
\subsection{Initial model}
With $n=1,\dots,4636$ and $m = 1,\dots,200$, the initial model is
$$
y_i \approx
\hat{y}_i = \sum_{j=1}^m \sum_{\Delta = 1}^{15}  M_{i,(j,\Delta)} \frac{x_{j}}{s_j}
.$$
That is, for each emission source $j$, we simulate the contribution of 15 days, and add this together for all $j$. A matrix formulation of this reads as follows:
\begin{align}
    \label{eq: initial model concise} 
\begin{bmatrix}
    \hat{y}_1 \\ 
    \vdots \\
    \hat{y}_{n}
\end{bmatrix}
&=
\begin{bmatrix}
    M_{1,(1,1)} & M_{1,(1,2)} & \cdots & M_{1,(m,15)} \\
    \vdots & \vdots & & \vdots \\
    M_{n,(1,1)} & M_{n,(1,2)} & \cdots & M_{n,(m,15)} \\
\end{bmatrix}
\begin{bmatrix}
    s_1^{-1} x_1\\
    \vdots \\
    s_1^{-1} x_1 \\
    s_2^{-1} x_2 \\
    \vdots \\
    s_n^{-1} x_m
\end{bmatrix} 
\\
&=
\begin{bmatrix}
    M_{1,(1,1)} & M_{1,(1,2)} & \cdots & M_{1,(n,15)} \\
    \vdots & \vdots & & \vdots \\
    M_{m,(1,1)} & M_{m,(1,2)} & \cdots & M_{m,(n,15)} \\
\end{bmatrix}
\left( 
     \begin{bmatrix}
        s_1 & & \\
         & \ddots & \\
        & & s_n
     \end{bmatrix}^{-1}
     \begin{bmatrix}
        x_1 \\
        \vdots \\
        x_n
     \end{bmatrix}
     \otimes \vb{1}_{15}
\right)
\nonumber
.\end{align}

Now we present a different interpretation. Instead of $S^{-1} x \otimes \vb{1}_{15}$, we make a vector $\tilde{x} := S^{-1} x \otimes \vb{1}_{365}$. This is a vector of length $365 \times 200$ that gives the emission from day 1 to day 365 for each source. This is assumed to be constant for all days. Therefore, $\tilde{x}$ consists of $m = 200$ blocks, each of which is a constant vector of length $365$.

We can write a system of equations that is logically the same as \cref{eq: initial model concise} but uses $\tilde{x}$. This gives a larger linear system. Because the emission from most dates has a negligible contribution to the observed concentrations, many coefficients in the larger matrix will be zero. For every measurement $i$, we have a linear relationship that looks something like this:
$$
        \hat{y}_i
    =
    \begin{bmatrix}
        \cdots & M_{i,(1,1)} & \dots & M_{i,(1,15)} & 0 & \dots & M_{i,(2,1)} & \dots & M_{i,(2,15)} & \dots
    \end{bmatrix}
    (S^{-1} x \otimes \vb{1}_{365})
.
$$
The presence of zeros in the matrix indicates that emissions from the distant past do not contribute to the observed concentrations.

Now assume that in each station, there is one observation per day, named $y_{kt}$, where $k = 1,\dots,K$ where $K$ is the number of stations and $t = 1,\dots,365$. The index $t=1$ corresponds to the most recent measurement. Then the full system reads as follows:
\begin{equation}
    \label{eq: initial model 365 days}
\begin{bmatrix}
    \hat{y}_{1,1} \\
    \hat{y}_{1,2} \\
    \vdots \\
    \hat{y}_{2,1} \\ 
    \vdots \\
    \hat{y}_{K,365}
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
    M_{(1,1), (1,1)} & M_{(1,1), (1,2)} & \cdots & M_{(1,1),(1,15)} & 0 & \cdots & \cdots & M_{(1,1), (2,1)} & \cdots & 0 \\
    0 & M_{(1,2), (1,1)} & \cdots & M_{(1,2), (1,14)} & M_{(1,2), (1,15)} & 0 & \cdots & 0 & \cdots & 0 \\
    \vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots & & \vdots \\
    M_{(2,1), (1,1)} & M_{(2,1), (1,2)} & \cdots & M_{(2,1),(1,15)} & 0 & \cdots & \cdots & M_{(2,1), (2,1)} & \cdots & 0 \\
    \vdots \\
    \vdots
\end{bmatrix}
}_{ := M_{shift}}
\tilde{x}
\end{equation}
where 
$$
\tilde{x} := S^{-1} x \otimes \vb{1}_{365}
=
\begin{bmatrix}
    s_1^{-1} x_1 \\
    \vdots \\
    s_1^{-1} x_1 \\ 
    s_2^{-1} x_2 \\
    \vdots \\ 
    s_m^{-1} x_m
\end{bmatrix}
.$$

We could reorganise the vector $\tilde{x}$ to the transposed expression $\vb{1}_{365} \otimes (S^{-1} x)$, which is a vector containing all emissions on day 1, followed by all emissions on day 2, etc. A similar permutation of $y$ could be chosen. This has the effect of permuting the rows and columns of $M_{shift}$. A permutation of the variables gives a model that is essentially the same as \cref{eq: initial model 365 days} but may have advantages when it comes to implementation or computational efficiency. We will not consider this.

\subsection{Model with time-dependent correction to the emissions}
If we weigh each day and each emission source by some factor, we are applying a transformation
$$
\begin{bmatrix}
    s_1^{-1} x_1 \\
    \vdots \\
    s_1^{-1} x_1 \\ 
    s_2^{-1} x_2 \\
    \vdots \\ 
    s_m^{-1} x_m
\end{bmatrix}
\mapsto
\begin{bmatrix}
    w_{1,1} s_1^{-1} x_1 \\
    \vdots \\
    w_{1,365} s_1^{-1} x_1 \\ 
    w_{2,1} s_2^{-1} x_2 \\
    \vdots \\ 
    w_{m, 365} s_m^{-1} x_m
\end{bmatrix}
$$
which can be written concisely as 
$$
W (S^{-1} x \otimes \vb{1}_{365})
\quad\text{where}\quad 
W = \begin{bmatrix}
    w_{1,1} & 0 & \cdots & 0 \\
    0 & w_{1,2} & \cdots & 0 \\
    \vdots & \vdots & \ddots & 0 \\
    0 & 0 & \cdots & w_{m, 365}
\end{bmatrix}
.$$
Using this weighted vector of emissions instead of $\tilde{x}$ in \cref{eq: initial model 365 days} gives
\begin{equation}
    \label{eq: time-evolving model concise}
\hat{y} = M_{shift} W (S^{-1} x \otimes \vb{1}_{365})
.\end{equation}
In this expression, the unknown is $W$ and should be fit to the data. 
% There are a plethora of numerical techniques for solving linear equations where the unknown is a matrix.
% In the following, we provide a theoretical description of a system that can be solved for $W$.

% We can put constraints on $W$ of the form $W \in \mathcal{W}$, where $\mathcal{W}$ is some set of constrained diagonal matrices. Suppose we have a parametrisation $\Gamma: \mathbb{R}^{k} \rightarrow \mathcal{W}, u \mapsto \Gamma(u)$. 
% In this case, \cref{eq: time-evolving model concise} can be reformulated as a linear operation applied to $\Gamma(u)$ as follows:
% $$
% \hat{y} = ((S^{-1} x \otimes \vb{1}_{365})^T \otimes M_{shift}) \mathrm{vec}\, \Gamma(u)
% .$$
% If $\Gamma$ is a linear map, the above is a linear expression in $u$. Then, the minimisation of $\norm{y - \hat{y}}^2$ can be solved using numerical linear algebra.

\section{Fitting the model}
\subsection{Regularisation}
The indices $i = 1,\dots,n$ are partitioned as a disjoint union $\mathcal{I}_{det} \cup \mathcal{I}_{0}$ where $\mathcal{I}_0$ contains the indices of non-detection events. Let $P_{det}$ be the linear map that removes the rows of a matrix that correspond to $\mathcal{I}_0$. We can measure the accuracy as $\norm{\log.(P_{det} \hat{y})}^2$ where $\log.(\cdot)$ is the elementwise \emph{natural} logarithm. The simplicity of $W$ can be measured as $\norm{\log W}^2$. If we forget about non-detections, we have the cost function 
$$
\min_{W \text{ diagonal}} f(W) =
\underbrace{\frac{1}{2}\norm{\log.(P_{det} M_{shift} W \tilde{x})}^2}_{\text{prediction error}} + \underbrace{\frac{1}{2}\lambda \norm{\log W}^2}_{\text{deviation from average emission}}
.$$
In order to optimise $f$ numerically, we need to be able to measure changes to $W$. We do this implicitly by parametrising $W = e^{\mathrm{diag}\, v}$ for some vector $v$. If we apply an absolute change $v' := v + \Delta v$ in numerical optimisation, this corresponds to the relative change $W' = W e^{\mathrm{diag} \Delta v}$. 
Reformulating the cost function gives 
$$
\min_{v} f(v) =
\underbrace{\frac{1}{2}\norm{\log.(P_{det} M_{shift} e^{\mathrm{diag}\, v} \tilde{x})}^2}_{\text{prediction error}} + \underbrace{\frac{1}{2}\lambda \norm{v}^2}_{\text{deviation from average emission}}
.$$



\begin{proposition}
    Let $\hat{z}$ be the vector $[\hat{z}_1\,\,\dots\,\,\hat{z}_n]^T$ where 
    $$
    \hat{z}_i = \begin{cases}
        \frac{\log \hat{y}_i}{\hat{y}_i} & \text{if observation $i$ is a detection} \\
        0 & \text{if observation $i$ is a non-detection} 
    \end{cases}
    .$$
    Then
    $$
    \mathrm{grad}_{v} f(v) = e^v \odot (M_{shift}^T \hat{z}) \odot \tilde{x} + \lambda v
    .$$
\end{proposition}
\begin{proof}

Let $U = \mathrm{vec} \circ \mathrm{diag}$. Then $\mathrm{vec}\, W(v) = U e^v$ where $e^v$ denotes elementwise exponentiation.
Define $$
G(v) :=
P_{det} \hat{y}_i
=
P_{det} M_{shift} W(v) \tilde{x}
\quad\text{and}\quad
F(v) := \log.(G(v))
.$$
Then 
$$
G(v) = P_{det} M_{shift} W(v) \tilde{x}
=
(\tilde{x}^T \otimes P_{det} M_{shift}) \mathrm{vec} W(v)
=
\underbrace{(\tilde{x}^T \otimes P_{det}M_{shift}) U}_A e^v
$$
so that 
$$
\frac{\partial G}{\partial v} = A e^{\mathrm{diag}\, v}
.$$
Differentiating $F(v) = \log.(G(v))$ gives:
$$
\frac{\partial F}{\partial v}
=
\frac{\partial}{\partial v} \log.(G(v))
=
\mathrm{diag}\,(G(v))^{-1} \frac{\partial G}{\partial v}
=
\mathrm{diag}\,(G(v))^{-1} A e^{\mathrm{diag}\, v }
.$$
Write $z := P_{det} \hat{z}$.
We can interpret $z$ as the vector obtained by applying the map $x \mapsto (\log x) / x$ elementwise to $G(v)$.
Using standard differentiation rules for the squared norm of a vector-valued function, we obtain
\begin{align*}
\mathrm{grad}_v\, \left(\frac{1}{2} \norm{F(v)}^2 \right) =\left(\frac{\partial F}{\partial v}\right)^T F(v)
&= e^{\mathrm{diag}\, v} A^T \mathrm{diag}(G(t))^{-1} \log.(G(t)) \\
&= e^{\mathrm{diag}\, v} A^T z \\
&= e^{\mathrm{diag}\, v} U^T (\tilde{x} \otimes (P_{det} M_{shift})^T) z \\
&= e^{\mathrm{diag}\, v} U^T (P_{det} M_{shift})^T z \tilde{x}^T \\
&= e^{\mathrm{diag}\, v} U^T M_{shift}^T P_{det}^T P_{det} \hat{z} \tilde{x}^T \\
&= e^{\mathrm{diag}\, v} U^T M_{shift}^T \hat{z} \tilde{x}^T
\end{align*}
where the last equality follows from the fact that $P_{det}^T P_{det}$ is the map that sets the rows corresponding to the non-detection events to zero. 
By construction of $U$, the map $U^T$ projects its argument orthogonally onto the diagonal matrices and gives the diagonal element. The orthogonal projection of a matrix onto the diagonal matrices is obtained by setting the off-diagonal elements to zero. The diagonal elements of $M_{shift}^T \hat{z} \tilde{x}^T$ are $(M_{shift}^T \hat{z}) \odot \tilde{x}$, so that the above simplifies to
$$
\mathrm{grad}_v\, \left(\frac{1}{2} \norm{F(v)}^2 \right)
=
e^{\mathrm{diag}\, v} \left((M_{shift}^T \hat{z}) \odot \tilde{x}\right)
=
e^v \odot (M_{shift}^T \hat{z}) \odot \tilde{x}
.$$

The foregoing gives the derivative of the first term in $f(W)$. The gradient of the second term can is simply $\lambda v$.

\end{proof}
For even more evidence, there is a Julia script in the repository that checks this formula against the result obtained from automatic differentiation (using simplified mock data). The result is correct up to machine precision.

\subsection{Constrained optimisation (similar idea as regularisation)}
Constrained optimisation is similar to using regularisation.
Suppose that the number of components in $\tilde{x}$ is $m T$ (where $m$ is the number of sources and $T$ is the number of days).
If we want to bound the average of $(\log w_{ij})^2$ by some value $\delta^2$, then we have the constrained optimisation problem
\begin{align*}
    \min\quad & \frac{1}{2} \norm{\log.(P_{det} M_{shift} W \tilde{x})}^2 \\
    \text{s.t.}\quad & \frac{1}{mT} \norm{\log W}^2 \le \delta^2
\end{align*} 
or equivalently 
\begin{align*}
    \min\quad & \frac{1}{2} \norm{\log.(P_{det} M_{shift} e^{\mathrm{diag}\, v} \tilde{x})}^2 \\
    \text{s.t.}\quad & \frac{1}{mT} \norm{v}^2 \le \delta^2
\end{align*}
which can be solved using very similar ideas as the regularisation section.

\section{Correcting for non-detections}

To correct for non-detections, we apply the transformation $h$ from \cite{deMeutter2022uncertainty}. This is given by the continuously differentiable function
$$
h(y) = \begin{cases}
    \frac{y^2}{4MDC} + MDC &\quad \text{if } y \le 2MDC \\
    y &\quad \text{else}
\end{cases}
.$$
This transformation maps the interval $[0, MDC]$ onto $[MDC, 2MDC]$. For observations that are significantly larger than $MDC$, this is approximately the identity. If $y$ and the simulated value $\hat{y}$ are both larger than $2MDC$, then $h(\hat{y}) / h(y) = \hat{y} / y$. If $y = 0$ because the concentration is not detectable, then we want a simulated value $\hat{y}$ such that $h(\hat{y}) \approx MDC$. In both cases, we can use the value of $h(\hat{y}) / h(y)$ as an indication of the accuracy of the simulation.


In the dataset that we are working with, we know the values of $- \frac{M_{i,(j,\Delta)}}{MDC}$ for every non-detection event. In these events, we work as if $MDC = 1$. If the predicted value is $\hat{y}_i < 0$, we have 
$$
h\left(\hat{y}\right) = \begin{cases}
    \frac{\hat{y}^2}{4} + 1 &\quad\text{if } \hat{y} \le -2 \\
    -\hat{y} &\quad\text{else}
\end{cases}
$$
and $h(y) = 1$. For the detection events, we do not have the $MDC$ and cannot calculate $h$ exactly. For the detection events, we can approximate $h$ by the identity map.

Now a good cost function is
$$
\min_{v} f(v) =
\underbrace{\frac{1}{2}\norm{\log h.(\hat{y})}^2}_{\text{prediction error}} + \underbrace{\frac{1}{2}\lambda \norm{v}^2}_{\text{deviation from average emission}}
\quad\text{where}\quad 
\hat{y} = M_{shift} e^{\mathrm{diag}\, v} \tilde{x}
$$
where $\log h.$ is the elementwise application of $x \mapsto \log h(x)$ to a vector.

\begin{proposition}
    Let $g$ be the vector defined elementwise by 
    $$
    g_i = \frac{\log h(\hat{y}_i)}{h(\hat{y}_i)} \frac{\mathrm{d}}{\mathrm{d}\hat{y}_i} h(\hat{y}_i)
    .$$
    Then 
    $$
    \mathrm{grad}\, f(v) = e^v \odot (M_{shift}^T g) \odot \tilde{x} 
    $$
\end{proposition}
\begin{proof}

Let $U = \mathrm{vec} \circ \mathrm{diag}$. Then $\mathrm{vec}\, W(v) = U e^v$ where $e^v$ denotes elementwise exponentiation.
Define $$
F(v) := \log h.(\hat{y}(v))
.$$
Since 
$$
\hat{y}(v) = M_{shift} W(v) \tilde{x}
=
(\tilde{x}^T \otimes M_{shift}) \mathrm{vec} W(v)
=
(\tilde{x}^T \otimes M_{shift}) U e^v
,$$
we have
$$
\frac{\partial F}{\partial v}
=
\mathrm{diag}(d \log h.(\hat{y}(v))) \frac{\partial \hat{y}}{\partial v}
=
\underbrace{\mathrm{diag}(d \log h.(\hat{y}(v)))}_H U^T (\tilde{x} \otimes M_{shift}^T) e^{\mathrm{diag}\, v}
.$$
Using standard differentiation rules for the squared norm of a vector-valued function, we obtain
\begin{align*}
\mathrm{grad}_v\, \left(\frac{1}{2} \norm{F(v)}^2 \right) =
\left(\frac{\partial F}{\partial v}\right)^T F(v)
&= e^{\mathrm{diag}\, v} U^T (\tilde{x} \otimes M_{shift}^T) H \log h.(\hat{y})
\end{align*}
Since $U$ is the unitary map that maps a vector onto (the vectorisation of) the corresponding diagonal matrix, $U^T$ maps the vectorisation of a matrix onto its diagonal. If $\mathrm{Diag}: \mathbb{R}^{p \times p} \rightarrow \mathbb{p}$ maps a matrix to its diagonal, we have 
\begin{align*}
\mathrm{grad}_v\, \left(\frac{1}{2} \norm{F(v)}^2 \right)
&=
e^{\mathrm{diag}\, v} \mathrm{Diag}(M_{shift}^T H \log h.(\hat{y}) \tilde{x}^T) \\
&=
e^{\mathrm{diag}\, v} \left((M_{shift}^T H \log h.(\hat{y})) \odot \tilde{x}\right) \\
&=
e^{v} \odot \left(M_{shift}^T H \log h.(\hat{y})\right) \odot \tilde{x} \\
&=
e^v \odot (M_{shift}^T g(\hat{y})) \odot \tilde{x}
\end{align*}
where $g$ is the elementwise application of $x \mapsto \log h(x) \frac{\mathrm{d}}{\mathrm{d}x}(\log h(x))$. Thus, 
$$
g(\hat{y}_i) = \frac{\log h(\hat{y}_i)}{h(\hat{y}_i)} \frac{\mathrm{d}}{\mathrm{d}\hat{y}_i} h(\hat{y}_i)
$$


The foregoing gives the derivative of the first term in $f(W)$. The gradient of the second term is simply $\lambda v$.


\end{proof}


\section{Further work}
\begin{itemize}
    \item For certain emission sources in certain periods, there is an expected correlation between the emission at time $t$ and at time $t + 1, t + 2, \dots$. We can add a constraint on the autocorrelation.
    \item Change the parametrisation to something like this: 
    \begin{align*}
        x_1^{2 \text{ Jan } 2014} &= \alpha_1^{(1)} x_1^{1 \text{ Jan } 2014} \\
        x_1^{3 \text{ Jan} 2014} &= \alpha_2^{(1)} x_1^{2 \text{ Jan } 2014} \\
        &\vdots
    \end{align*}
    with a constraint like $\alpha_{\min} \le \prod_{t=1}^T \alpha_t^{(j)} \le \alpha_{\max}$ for all $T$ and all $j = 1,\dots,m$. However, this would be difficult to implement. In addition, the complexity of inequality constrained optimisation can be combinatorial in the number of constraints if an active set method is used \cite{Nocedal2006}.

    \item How do you store the sparse matrix $M_{shift}$ in a computer? Most programming languages have data structures for large sparse matrices?
\end{itemize}

\printbibliography

\end{document}

