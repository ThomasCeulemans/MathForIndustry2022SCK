\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, dsfont, physics, hyperref, cleveref}
\usepackage[margin=1.5cm]{geometry}
\usepackage{biblatex}

\title{Notes}
\author{SCK team}
\date{}

\newtheorem{proposition}{Proposition}

\addbibresource{library.bib}

\begin{document}
    \maketitle

\section{Notation}
\begin{itemize}
    \item Average emission per source: $x_1,\dots,x_m$ where $m = 200$.
    \item Measured concentrations $y_1,\dots,y_n$ where $n = 4636$. These are consecutive measurements for several measurement stations. 
    \item $M_{i,(j,\Delta)}$ is a block in the sensitivity matrix $M$ that represents the contribution of $x_j$ from $\Delta - 1$ days ago to the observed concentration $y_i$.
    \item Scaling factors $s_1,\dots,s_m$ (in \texttt{scalings.csv}). These are put in a diagonal matrix $S$.
    \item $\vb{1}_q = [1\,\, \dots\,\, 1]^T \in \mathbb{R}^q$ is the constant vector consisting of only ones.
    \item $\odot$ is the Hadamard (i.e.\, elementwise) product, given by $(AB)_{ij} = A_{ij} B_{ij}$.
    \item $\mathrm{vec}: \mathbb{R}^{p \times q} \rightarrow \mathbb{R}^{pq}$ is the vectorisation operator. It can be understood as stacking the columns of a matrix in one long vector. 
    \item $\mathrm{diag}$ is the canonical identification of $p$-vectors and $p \times p$ diagonal matrices, i.e.,
    $$
    \mathrm{diag}\left(
    \begin{bmatrix}
        a_1 \\ 
        \vdots \\
        a_p
    \end{bmatrix}
    \right)
    =
    \begin{bmatrix}
        a_1 & 0 & \dots & 0 \\
        0 & a_2 & \dots & 0 \\
        \vdots & \vdots & \ddots & 0 \\
        0 & 0 & \cdots & a_p
    \end{bmatrix}
    .$$
    \item $\otimes$ is the Kronecker product. In particular,
        $$
        \underbrace{
        \begin{bmatrix}
            a_1 \\
            \vdots \\
            a_p
        \end{bmatrix}
        }_{\in \mathbb{R}^{p}}
        \otimes 
        \underbrace{
        \begin{bmatrix}
            1 \\
            \vdots \\
            1
        \end{bmatrix}
        }_{\in \mathbb{R}^{q}}
        =
        \underbrace{
        \begin{bmatrix}
            a_1 \\
            \vdots \\
            a_1 \\
            a_2 \\
            \vdots \\
            a_2 \\
            \vdots \\
            a_p
        \end{bmatrix}
        }_{\in \mathbb{R}^{pq}}
        .$$
        A standard rule of thumb in numerical linear algebra is that the Kronecker product serves a mostly theoretical purpose and is rarely calculated explicitly.
        This is because the Kronecker product has the following convenient property \cite[Section 12.3]{Golub2013}: $\mathrm{vec} (AXB) = (B^T \otimes A) \mathrm{vec} X$ for all matrices $A,X,B$.
    \item For a vector-valued function $F(x)$, the Jacobian matrix is denoted as $\frac{\partial F}{\partial x}$.
\end{itemize}

\section{Model}
\subsection{Initial model}
With $n=1,\dots,4636$ and $m = 1,\dots,200$, the initial model is
$$
y_i \approx
\hat{y}_i = \sum_{j=1}^m \sum_{\Delta = 1}^{15}  M_{i,(j,\Delta)} \frac{x_{j}}{s_j}
.$$
That is, for each emission source $j$, we simulate the contribution of 15 days, and add this together for all $j$. A matrix formulation of this reads as follows:
\begin{align}
    \label{eq: initial model concise} 
\begin{bmatrix}
    \hat{y}_1 \\ 
    \vdots \\
    \hat{y}_{n}
\end{bmatrix}
&=
\begin{bmatrix}
    M_{1,(1,1)} & M_{1,(1,2)} & \cdots & M_{1,(m,15)} \\
    \vdots & \vdots & & \vdots \\
    M_{n,(1,1)} & M_{n,(1,2)} & \cdots & M_{n,(m,15)} \\
\end{bmatrix}
\begin{bmatrix}
    s_1^{-1} x_1\\
    \vdots \\
    s_1^{-1} x_1 \\
    s_2^{-1} x_2 \\
    \vdots \\
    s_n^{-1} x_m
\end{bmatrix} 
\\
&=
\begin{bmatrix}
    M_{1,(1,1)} & M_{1,(1,2)} & \cdots & M_{1,(n,15)} \\
    \vdots & \vdots & & \vdots \\
    M_{m,(1,1)} & M_{m,(1,2)} & \cdots & M_{m,(n,15)} \\
\end{bmatrix}
\left( 
     \begin{bmatrix}
        s_1 & & \\
         & \ddots & \\
        & & s_n
     \end{bmatrix}^{-1}
     \begin{bmatrix}
        x_1 \\
        \vdots \\
        x_n
     \end{bmatrix}
     \otimes \vb{1}_{15}
\right)
\nonumber
.\end{align}

Now we present a different interpretation. Instead of $S^{-1} x \otimes \vb{1}_{15}$, we make a vector $\tilde{x} := S^{-1} x \otimes \vb{1}_{365}$. This is a vector of length $365 \times 200$ that gives the emission from day 1 to day 365 for each source. This is assumed to be constant for all days. Therefore, $\tilde{x}$ consists of $m = 200$ blocks, each of which is a constant vector of length $365$.

We can write a system of equations that is logically the same as \cref{eq: initial model concise} but uses $\tilde{x}$. This gives a larger linear system. Because the emission from most dates has a negligible contribution to the observed concentrations, many coefficients in the larger matrix will be zero. For every measurement $i$, we have a linear relationship that looks something like this:
$$
        \hat{y}_i
    =
    \begin{bmatrix}
        \cdots & M_{i,(1,1)} & \dots & M_{i,(1,15)} & 0 & \dots & M_{i,(2,1)} & \dots & M_{i,(2,15)} & \dots
    \end{bmatrix}
    (S^{-1} x \otimes \vb{1}_{365})
.
$$
The presence of zeros in the matrix indicates that emissions from the distant past do not contribute to the observed concentrations.

Now assume that in each station, there is one observation per day, named $y_{kt}$, where $k = 1,\dots,K$ where $K$ is the number of stations and $t = 1,\dots,365$. The index $t=1$ corresponds to the most recent measurement. Then the full system reads as follows:
\begin{equation}
    \label{eq: initial model 365 days}
\begin{bmatrix}
    \hat{y}_{1,1} \\
    \hat{y}_{1,2} \\
    \vdots \\
    \hat{y}_{2,1} \\ 
    \vdots \\
    \hat{y}_{K,365}
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
    M_{(1,1), (1,1)} & M_{(1,1), (1,2)} & \cdots & M_{(1,1),(1,15)} & 0 & \cdots & \cdots & M_{(1,1), (2,1)} & \cdots & 0 \\
    0 & M_{(1,2), (1,1)} & \cdots & M_{(1,2), (1,14)} & M_{(1,2), (1,15)} & 0 & \cdots & 0 & \cdots & 0 \\
    \vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots & & \vdots \\
    M_{(2,1), (1,1)} & M_{(2,1), (1,2)} & \cdots & M_{(2,1),(1,15)} & 0 & \cdots & \cdots & M_{(2,1), (2,1)} & \cdots & 0 \\
    \vdots \\
    \vdots
\end{bmatrix}
}_{ := M_{shift}}
\tilde{x}
\end{equation}
where 
$$
\tilde{x} := S^{-1} x \otimes \vb{1}_{365}
=
\begin{bmatrix}
    s_1^{-1} x_1 \\
    \vdots \\
    s_1^{-1} x_1 \\ 
    s_2^{-1} x_2 \\
    \vdots \\ 
    s_m^{-1} x_m
\end{bmatrix}
.$$

We could reorganise the vector $\tilde{x}$ to the transposed expression $\vb{1}_{365} \otimes (S^{-1} x)$, which is a vector containing all emissions on day 1, followed by all emissions on day 2, etc. A similar permutation of $y$ could be chosen. This has the effect of permuting the rows and columns of $M_{shift}$. A permutation of the variables gives a model that is essentially the same as \cref{eq: initial model 365 days} but may have advantages when it comes to implementation or computational efficiency. We will not consider this.

\subsection{Model with time-dependent correction to the emissions}
If we weigh each day and each emission source by some factor, we are applying a transformation
$$
\begin{bmatrix}
    s_1^{-1} x_1 \\
    \vdots \\
    s_1^{-1} x_1 \\ 
    s_2^{-1} x_2 \\
    \vdots \\ 
    s_m^{-1} x_m
\end{bmatrix}
\mapsto
\begin{bmatrix}
    w_{1,1} s_1^{-1} x_1 \\
    \vdots \\
    w_{1,365} s_1^{-1} x_1 \\ 
    w_{2,1} s_2^{-1} x_2 \\
    \vdots \\ 
    w_{m, 365} s_m^{-1} x_m
\end{bmatrix}
$$
which can be written concisely as 
$$
W (S^{-1} x \otimes \vb{1}_{365})
\quad\text{where}\quad 
W = \begin{bmatrix}
    w_{1,1} & 0 & \cdots & 0 \\
    0 & w_{1,2} & \cdots & 0 \\
    \vdots & \vdots & \ddots & 0 \\
    0 & 0 & \cdots & w_{m, 365}
\end{bmatrix}
.$$
Using this weighted vector of emissions instead of $\tilde{x}$ in \cref{eq: initial model 365 days} gives
\begin{equation}
    \label{eq: time-evolving model concise}
\hat{y} = M_{shift} W (S^{-1} x \otimes \vb{1}_{365})
.\end{equation}
In this expression, the unknown is $W$ and should be fit to the data. 


\section{Correction for non-detections}

To correct for non-detections, we apply the transformation $h$ from \cite{deMeutter2022uncertainty}. This is given by the continuously differentiable function
$$
h(y) = \begin{cases}
    \frac{y^2}{4MDC} + MDC &\quad \text{if } y \le 2MDC \\
    y &\quad \text{else}
\end{cases}
.$$
This transformation maps the interval $[0, MDC]$ onto $[MDC, 2MDC]$. For observations that are significantly larger than $MDC$, this is approximately the identity. If $y$ and the simulated value $\hat{y}$ are both larger than $2MDC$, then $h(\hat{y}) / h(y) = \hat{y} / y$. If $y = 0$ because the concentration is not detectable, then we want a simulated value $\hat{y}$ such that $h(\hat{y}) \approx MDC$. In both cases, we can use the value of $h(\hat{y}) / h(y)$ as an indication of the accuracy of the simulation.


In the dataset that we are working with, we know the values of $- \frac{M_{i,(j,\Delta)}}{MDC}$ for every non-detection event. In these events, we work as if $MDC = 1$. If the predicted value is $\hat{y}_i < 0$, we have 
$$
h\left(\hat{y}\right) = \begin{cases}
    \frac{\hat{y}^2}{4} + 1 &\quad\text{if } \hat{y} \le -2 \\
    -\hat{y} &\quad\text{else}
\end{cases}
$$
and $h(y) = 1$. For the detection events, we do not have the $MDC$ and cannot calculate $h$ exactly. For the detection events, we can approximate $h$ by the identity map.

\section{Fitting the model}
In the above, we argued that, the prediction is accurate if $h(\hat{y})$ is close to one, where $\hat{y}$ is the predicted concentration. This is equivalent to $\log^2(h(\hat{y}))$ being small. Notice that this penalises overestimates by a factor $\alpha$ as much as underestimates by a factor $\alpha$. That is, 
$$
\log^2(\alpha) = \log^2(\alpha^{-1})
.$$
The average prediction error over all predictions is $\sum_{i=1}^n \log^2(h(\hat{y}_i)) = \norm{\log h(\hat{y})}^2$. This will be the first criterion for optimality of the model.

The second criterion is the ratio between the daily emissions and the average emission for each source, i.e. the factors $W$. In a similar vein as the above, the size of $W$ can be measured as $\norm{\log W}^2$ where $\norm{\cdot}$ is the Frobenius norm\footnote{Since $W$ is a diagonal matrix, the matrix logarithm $\log W$ is simply the diagonal matrix with $\log w_{1,1}, \log w_{1,2}, \dots$ on the diagonal. Hence, $\norm{\log W}^2 = \log^2 w_{1,1} + \log^2 w_{1,2} + \dots$}. 

To balance these two criteria, we measure the quality of the model as 
\begin{equation}
    \label{eq: cost function regularised}
f(W) := 
\underbrace{\frac{1}{2}\norm{\log h.(\hat{y})}^2}_{\text{prediction error}} + \underbrace{\frac{1}{2}\lambda \norm{\log W}^2}_{\text{deviation from average emission}}
\quad\text{where}\quad 
\hat{y} = M_{shift} W \tilde{x}
\end{equation}
where $\lambda \ge 0$ is a regularisation parameter.

To optimise $f$ numerically, we will parametrise $W$ as $W = e^{\mathrm{diag}\, v}$ where $v = [v_1, v_2,\dots]$ is some vector and $e^{\mathrm{diag}\, v}$ is the diagonal matrix with $[e^{v_1}, e^{v_2},\dots]$ on the diagonal. The motivation for doing this is twofold:
\begin{enumerate}
    \item It ensures that $W$ remains strictly positive. In particular, the diagonal elements of $W$ can only tend towards zero if $v_j \rightarrow -\infty$ for some $i$. 
    \item It implicitly measures changes to $W$ in a relative sense. If the optimisation algorithm performs a linear update $v_{k+1} = v_k + \Delta v$, this corresponds to a relative update $W_{k+1} = W_k e^{\mathrm{diag}\, \Delta v}$.
\end{enumerate}

Thus, in practice, we are interested in the optimisation problem
$$
\min_{v} f(v) =
\frac{1}{2}\norm{\log h.(\hat{y})}^2 + \frac{1}{2}\lambda \norm{v}^2
\quad\text{where}\quad 
\hat{y} = M_{shift} e^{\mathrm{diag}\, v} \tilde{x}
.$$
Most optimisation algorithms require at least the gradient of $f$. This is given by the following:
\begin{proposition}
    Let $g = [g_1, \dots, g_n]$ be the vector defined elementwise by 
    $$
    g_i = \frac{\log h(\hat{y}_i)}{h(\hat{y}_i)} \frac{\mathrm{d}}{\mathrm{d}\hat{y}_i} h(\hat{y}_i)
    .$$
    Then 
    $$
    \mathrm{grad}\, f(v) = e^v \odot (M_{shift}^T g) \odot \tilde{x} 
    $$
    where $e^v$ denotes elementwise exponentiation and $\odot$ is the Hadamard (elementwise) product.
\end{proposition}
\begin{proof}

Let $U = \mathrm{vec} \circ \mathrm{diag}$. Then $\mathrm{vec}\, W(v) = U e^v$ where $e^v$ denotes elementwise exponentiation.
Define $$
F(v) := \log h.(\hat{y}(v))
.$$
Since 
$$
\hat{y}(v) = M_{shift} W(v) \tilde{x}
=
(\tilde{x}^T \otimes M_{shift}) \mathrm{vec} W(v)
=
(\tilde{x}^T \otimes M_{shift}) U e^v
,$$
we have
$$
\frac{\partial F}{\partial v}
=
\mathrm{diag}(d \log h.(\hat{y}(v))) \frac{\partial \hat{y}}{\partial v}
=
\underbrace{\mathrm{diag}(d \log h.(\hat{y}(v)))}_H U^T (\tilde{x} \otimes M_{shift}^T) e^{\mathrm{diag}\, v}
.$$
Using standard differentiation rules for the squared norm of a vector-valued function, we obtain
\begin{align*}
\mathrm{grad}_v\, \left(\frac{1}{2} \norm{F(v)}^2 \right) =
\left(\frac{\partial F}{\partial v}\right)^T F(v)
&= e^{\mathrm{diag}\, v} U^T (\tilde{x} \otimes M_{shift}^T) H \log h.(\hat{y})
\end{align*}
Since $U$ is the unitary map that maps a vector onto (the vectorisation of) the corresponding diagonal matrix, $U^T$ maps the vectorisation of a matrix onto its diagonal. If $\mathrm{Diag}: \mathbb{R}^{p \times p} \rightarrow \mathbb{p}$ maps a matrix to its diagonal, we have 
\begin{align*}
\mathrm{grad}_v\, \left(\frac{1}{2} \norm{F(v)}^2 \right)
&=
e^{\mathrm{diag}\, v} \mathrm{Diag}(M_{shift}^T H \log h.(\hat{y}) \tilde{x}^T) \\
&=
e^{\mathrm{diag}\, v} \left((M_{shift}^T H \log h.(\hat{y})) \odot \tilde{x}\right) \\
&=
e^{v} \odot \left(M_{shift}^T H \log h.(\hat{y})\right) \odot \tilde{x} \\
&=
e^v \odot (M_{shift}^T g(\hat{y})) \odot \tilde{x}
\end{align*}
where $g$ is the elementwise application of $x \mapsto \log h(x) \frac{\mathrm{d}}{\mathrm{d}x}(\log h(x))$. Thus, 
$$
g(\hat{y}_i) = \frac{\log h(\hat{y}_i)}{h(\hat{y}_i)} \frac{\mathrm{d}}{\mathrm{d}\hat{y}_i} h(\hat{y}_i)
$$


The foregoing gives the derivative of the first term in $f(W)$. The gradient of the second term is simply $\lambda v$.


\end{proof}


\section{Further work}
\begin{itemize}
    \item For certain emission sources in certain periods, there is an expected correlation between the emission at time $t$ and at time $t + 1, t + 2, \dots$. We can add a constraint on the autocorrelation.
    \item Time-correlation could be measured by parameterising the model as follows: 
    \begin{align*}
        x_1^{2 \text{ Jan } 2014} &= \alpha_1^{(1)} x_1^{1 \text{ Jan } 2014} \\
        x_1^{3 \text{ Jan} 2014} &= \alpha_2^{(1)} x_1^{2 \text{ Jan } 2014} \\
        &\vdots
    \end{align*}
    with a constraint like $\alpha_{\min} \le \prod_{t=1}^T \alpha_t^{(j)} \le \alpha_{\max}$ for all $T$ and all $j = 1,\dots,m$. However, this would be difficult to implement. In addition, the complexity of inequality constrained optimisation can be combinatorial in the number of constraints if an active set method is used \cite{Nocedal2006}.

    \item For the sake of simplicity, the current implementation does not store $M_{shift}$ in memory in a data structure for large and sparse matrices. This could be changed to improve memory use and/or computation time.
    
    \item Other numerical optimisation algorithms may be more efficient or have better convergence properties. For instance, the Gauss-Newton and Levenberg-Marquardt methods are specifically designed for optimising the squared norm of a vector-valued function, but may require a large amount of memory \cite[Section 10.3]{Nocedal2006}.
    
    \item Standard software libraries support optimisation with constraints. This is an alternative to regularisation and may be more interpretable.
    Suppose that the number of components in $\tilde{x}$ is $m T$ (where $m$ is the number of sources and $T$ is the number of days).
    If we want to bound the average of $(\log w_{ij})^2$ by some value $\delta^2$, then we have the constrained optimisation problem
    \begin{align*}
        \min\quad & \frac{1}{2} \norm{\log h.(\hat{y})}^2 \\
        \text{s.t.}\quad & \frac{1}{mT} \norm{\log W}^2 \le \delta^2
    \end{align*} 
    which can be solved using similar ideas as those developed above. The prototypical constrained optimisation method (such as an active set method) would iterate towards a critical point of the Lagrangian
    $$
    \mathcal{L}(W, \lambda) = \frac{1}{2} \norm{\log h.(\hat{y})}^2 + \lambda\left(\frac{1}{mT} \norm{\log W}^2 - \delta^2\right)
    $$
    in which case the computational cost would be comparable to optimising \cref{eq: cost function regularised}. A more detailed explanation of constrained optimisation and the Lagrangian is found in \cite{Nocedal2006}.
\end{itemize}

\printbibliography

\end{document}

